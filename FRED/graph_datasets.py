# AUTOGENERATED! DO NOT EDIT! File to edit: 01a01c Directed Stochastic Block Model.ipynb (unless otherwise specified).

__all__ = ['EmailEuNetwork', 'SourceSink', 'SmallRandom', 'ChainGraph', 'HalfChainGraph', 'CycleGraph',
           'HalfCycleGraph', 'DirectedStochasticBlockModel', 'source_graph', 'sink_graph', 'visualize_graph',
           'visualize_heatmap', 'display_heatmap_galary', 'display_graph_galary']

# Cell
import os
import torch
from torch_geometric.data import Data, InMemoryDataset, download_url, extract_gz
from torch_geometric.utils import sort_edge_index


class EmailEuNetwork(InMemoryDataset):
    """
    email-Eu-core network from Stanford Large Network Dataset Collection

    The network was generated from the email exchanges within a large European research institution.
    Each node represents an individual, and a directional edge from one individual to another represents some email exchanges between them in the specified direction.
    Each individual belongs to exactly one of 42 departments in the institution.
    """

    def __init__(self, transform=None, pre_transform=None):
        super().__init__("./datasets/email_Eu_network", transform, pre_transform)
        self.data, self.slices = torch.load(self.processed_paths[0])

    @property
    def raw_file_names(self):
        return ["email-Eu-core.txt", "email-Eu-core-department-labels.txt"]

    @property
    def processed_file_names(self):
        pre_transformed = "" if self.pre_transform is None else "_pre-transformed"
        return [f"email-Eu-network{pre_transformed}.pt", "never-skip-processing"]

    def download(self):
        for filename in self.raw_file_names:
            download_url(f"https://snap.stanford.edu/data/{filename}.gz", self.raw_dir)
            extract_gz(f"{self.raw_dir}/{filename}.gz", self.processed_dir)
            os.remove(f"{self.raw_dir}/{filename}.gz")

    def process(self):
        # Graph connectivity
        with open(self.raw_paths[0], "r") as f:
            edge_array = [
                [int(x) for x in line.split()] for line in f.read().splitlines()
            ]
        edge_index = torch.t(torch.tensor(edge_array))
        edge_index = sort_edge_index(edge_index)
        # Ground-truth label
        with open(self.raw_paths[1], "r") as f:
            label_array = [
                [int(x) for x in line.split()] for line in f.read().splitlines()
            ]
        y = torch.tensor(label_array)
        # Node identity features
        x = torch.eye(y.size(0), dtype=torch.float)
        # Build and save data
        data = Data(x=x, edge_index=edge_index, y=y)
        if self.pre_transform is not None:
            data = self.pre_transform(data)
        self.data, self.slices = self.collate([data])
        torch.save((self.data, self.slices), self.processed_paths[0])

# Cell
import warnings
from torch_geometric.transforms import BaseTransform
from torch_geometric.utils import sort_edge_index


class SourceSink(BaseTransform):
    """
    Transform a (directed or undirected) graph into a directed graph
    with a proportion of the nodes with mostly out-edges
    and a porportion of the nodes with mostly in-edges

    Parameters
    ----------
    prob_source : float
        must be between 0 and 1
        Proportion of nodes/communities to turn into source nodes/communities
        (with mostly out-edges)
    prob_sink : float
        must be between 0 and 1
        prob_source and prob_sink must add up to no more than 1
        Proportion of nodes/communities to turn into sink nodes/communities
        (with mostly in-edges)
    adv_prob : float
        must be between 0 and 1
        Probability of in-edges for source nodes and/or out-edges for sink nodes
    remove_prob : float
        must be between 0 and 1
        Probability of removing an in-edge for source nodes and/or out-edges for sink nodes
        1 - remove_prob is the probability of reversing the direction of in-edge for source nodes and/or out-edges for sink nodes
    """

    def __init__(self, prob_source=0.1, prob_sink=0.1, adv_prob=0, remove_prob=0):
        if prob_source + prob_sink > 1:
            warnings.warn("Total probability of source and sink exceeds 1")
            excess = prob_source + prob_sink - 1
            prob_source -= excess / 2
            prob_sink -= excess / 2
            warnings.warn(
                f"Adjusted: prob_source = {prob_source}, prob_sink = {prob_sink}"
            )
        self.prob_source = prob_source
        self.prob_sink = prob_sink
        self.adv_prob = adv_prob
        self.remove_prob = remove_prob

    def _has_ground_truth(self, data):
        return data.y is not None and data.y.shape == (data.num_nodes, 2)

    def _wrong_direction(self, labels, sources, sinks, tail, head):
        return (labels[head] in sources and labels[tail] not in sources) or (
            labels[tail] in sinks and labels[head] not in sinks
        )

    def __call__(self, data):
        if self._has_ground_truth(data):
            # get ground truth labels
            y = data.y[torch.argsort(data.y[:, 0]), :]
            classes = y[:, 1].unique()
            # randomly choose source and sink classes
            mask = torch.rand(len(classes))
            source_classes = classes[mask < self.prob_source]
            sink_classes = classes[mask > 1 - self.prob_sink]
            # add source/sink ground-truth label
            y = torch.hstack(
                (
                    y,
                    torch.t(
                        torch.tensor(
                            [
                                [
                                    1
                                    if c in source_classes
                                    else -1
                                    if c in sink_classes
                                    else 0
                                    for c in y[:, 1]
                                ]
                            ]
                        )
                    ),
                )
            )
            labels = y[:, 1]
            sources = source_classes
            sinks = sink_classes
        else:
            warnings.warn("Data has no ground-truth labels")
            # randomly choose source and sink nodes
            nodes = torch.arange(data.num_nodes)
            mask = torch.rand(data.num_nodes)
            source_nodes = nodes[mask < self.prob_source]
            sink_nodes = nodes[mask > 1 - self.prob_sink]
            # add source/sink ground-truth label
            y = torch.tensor(
                [
                    [n, 1 if n in source_nodes else -1 if n in sink_nodes else 0]
                    for n in nodes
                ]
            )
            labels = nodes
            sources = source_nodes
            sinks = sink_nodes

        # correct improper edges
        edge_array = []
        for e in range(data.num_edges):
            tail, head = data.edge_index[:, e]
            if (
                self._wrong_direction(labels, sources, sinks, tail, head)
                and torch.rand(1)[0] > self.adv_prob
            ):
                if torch.rand(1)[0] < self.remove_prob:  # remove the improper edge
                    continue
                else:  # reverse the improper edge
                    edge_array.append([head, tail])
            else:  # keep proper edge
                edge_array.append([tail, head])
        edge_index = torch.t(torch.tensor(edge_array))
        data.edge_index = sort_edge_index(edge_index)
        data.y = y
        return data.coalesce()

# Cell
import warnings
import torch
from torch_geometric.data import Data, InMemoryDataset
from torch_sparse import SparseTensor
from torch_geometric.utils import remove_self_loops


class SmallRandom(InMemoryDataset):
    def __init__(self, num_nodes=5, prob_edge=0.2, transform=None, pre_transform=None):
        super().__init__(".", transform, pre_transform)

        if num_nodes > 300:
            num_nodes = 300
            warnings.warn(
                f"Number of nodes is too large for SmallRandom dataset. Reset num_nodes =  {num_nodes}"
            )

        dense_adj = (torch.rand((num_nodes, num_nodes)) < prob_edge).int()
        sparse_adj = SparseTensor.from_dense(dense_adj)
        row, col, _ = sparse_adj.coo()
        edge_index, _ = remove_self_loops(torch.stack([row, col]))

        x = torch.eye(num_nodes, dtype=torch.float)
        data = Data(x=x, edge_index=edge_index)
        if self.pre_transform is not None:
            data = self.pre_transform(data)
        self.data, self.slices = self.collate([data])

# Cell
class ChainGraph(InMemoryDataset):
    def __init__(self, num_nodes=2, transform=None):
        super().__init__(".", transform)
        dense_adj = torch.diag(torch.ones(num_nodes-1), 1)
        sparse_adj = SparseTensor.from_dense(dense_adj)
        row, col, _ = sparse_adj.coo()
        edge_index, _ = remove_self_loops(torch.stack([row, col]))

        x = torch.eye(num_nodes, dtype=torch.float)
        data = Data(x=x, edge_index=edge_index)
        self.data, self.slices = self.collate([data])

# Cell
class HalfChainGraph(InMemoryDataset):
    def __init__(self, num_nodes=3, center=1, transform=None):
        super().__init__(".", transform)
        dense_adj = (
            torch.diag(torch.cat((torch.ones(center), torch.zeros(num_nodes-center-1))), 1)
            + torch.diag(torch.cat((torch.zeros(center), torch.ones(num_nodes-center-1))), -1)
        )
        sparse_adj = SparseTensor.from_dense(dense_adj)
        row, col, _ = sparse_adj.coo()
        edge_index, _ = remove_self_loops(torch.stack([row, col]))

        x = torch.eye(num_nodes, dtype=torch.float)
        data = Data(x=x, edge_index=edge_index)
        self.data, self.slices = self.collate([data])

# Cell
class CycleGraph(InMemoryDataset):
    def __init__(self, num_nodes=3, transform=None):
        super().__init__(".", transform)
        dense_adj = torch.diag(torch.ones(num_nodes-1), 1)
        dense_adj[num_nodes-1,0] = 1
        sparse_adj = SparseTensor.from_dense(dense_adj)
        row, col, _ = sparse_adj.coo()
        edge_index, _ = remove_self_loops(torch.stack([row, col]))

        x = torch.eye(num_nodes, dtype=torch.float)
        data = Data(x=x, edge_index=edge_index)
        self.data, self.slices = self.collate([data])

# Cell
class HalfCycleGraph(InMemoryDataset):
    def __init__(self, num_nodes=3, center=0, transform=None):
        super().__init__(".", transform)
        dense_adj = torch.diag(torch.ones(num_nodes-1), 1)
        dense_adj[num_nodes-1,0] = 1
        dense_adj[(center+1)%num_nodes, center] = 1
        sparse_adj = SparseTensor.from_dense(dense_adj)
        row, col, _ = sparse_adj.coo()
        edge_index, _ = remove_self_loops(torch.stack([row, col]))

        x = torch.eye(num_nodes, dtype=torch.float)
        data = Data(x=x, edge_index=edge_index)
        self.data, self.slices = self.collate([data])

# Cell
import torch
import numpy as np
from torch_geometric.data import Data, InMemoryDataset
from torch_sparse import SparseTensor
from torch_geometric.utils import remove_self_loops


class DirectedStochasticBlockModel(InMemoryDataset):
    def __init__(self, num_nodes, num_clusters, aij, bij, transform=None):
        """Directed SBM

        Parameters
        ----------
        num_nodes : int
            _description_
        num_clusters : int
            must evenly divide num_nodes
        aij : num_nodes x num_nodes ndarray
            Probabilities of (undirected) connection between clusters i and j.
            Must be symmetric.
        bij : num_nodes x num_nodes ndarray
            Probabilities with which the edges made via aij are converted to directed edges.
            bij + bji = 1
        transform : _type_, optional
            _description_, by default None
        """
        super().__init__(".", transform)
        cluster = np.repeat(list(range(num_clusters)), num_nodes / num_clusters)
        rand_matrix = torch.rand((num_nodes, num_nodes))
        dense_adj = torch.empty((num_nodes, num_nodes))
        ## Draw inter-cluster undirected edges
        # inefficiently traverse the dense matrix, converting probabilities
        for i in range(num_nodes):
            for j in range(i, num_nodes):
                dense_adj[i, j] = (
                    1 if rand_matrix[i, j] < aij[cluster[i], cluster[j]] else 0
                )
                dense_adj[j, i] = dense_adj[i, j]  # it's symmetric
        ## Convert undirected edges to directed edges
        rand_matrix = torch.rand((num_nodes, num_nodes))
        for i in range(num_nodes):
            for j in range(i, num_nodes):
                # if an edge exists, assign it a direction
                if dense_adj[i, j] == 1:
                    if rand_matrix[i, j] < bij[cluster[i], cluster[j]]:
                        dense_adj[i, j] = 1
                        dense_adj[j, i] = 0
                    else:
                        dense_adj[i, j] = 0
                        dense_adj[j, i] = 1

        sparse_adj = SparseTensor.from_dense(dense_adj)
        row, col, _ = sparse_adj.coo()
        edge_index, _ = remove_self_loops(torch.stack([row, col]))

        x = torch.eye(num_nodes, dtype=torch.float)
        data = Data(x=x, edge_index=edge_index)
        self.data, self.slices = self.collate([data])

# Cell
def source_graph(n_points=700, num_clusters=7):
    # we'll start with 7 clusters; six on the outside, one on the inside
    aij = np.zeros((num_clusters, num_clusters))
    aij[0, :] = 0.9
    aij[:, 0] = 0.9
    np.fill_diagonal(aij, 0.9)
    bij = np.zeros((num_clusters, num_clusters))
    bij[0, :] = 1.0
    bij[:, 0] = 0.0
    np.fill_diagonal(bij, 0.5)
    dataset = DirectedStochasticBlockModel(
        num_nodes=n_points, num_clusters=num_clusters, aij=aij, bij=bij
    )
    return dataset

# Cell
def sink_graph(n_points=700, num_clusters=7):
    # we'll start with 7 clusters; six on the outside, one on the inside
    aij = np.zeros((num_clusters, num_clusters))
    aij[0, :] = 0.9
    aij[:, 0] = 0.9
    np.fill_diagonal(aij, 0.9)
    bij = np.zeros((num_clusters, num_clusters))
    bij[0, :] = 0.0
    bij[:, 0] = 1.0
    np.fill_diagonal(bij, 0.5)
    dataset = DirectedStochasticBlockModel(
        num_nodes=n_points, num_clusters=num_clusters, aij=aij, bij=bij
    )
    return dataset

# Comes from 01c Plotting Utils.ipynb, cell
import matplotlib.pyplot as plt
import networkx as nx
from torch_geometric.utils import to_networkx


def visualize_graph(data, is_networkx=False, to_undirected=False, ax=None):
    G = data if is_networkx else to_networkx(data, to_undirected=to_undirected)
    pos = nx.spring_layout(G, seed=42)
    if ax is None:
        nx.draw_networkx(
            G, pos=pos, arrowsize=20, node_color="#adade0"
        )
        plt.show()
    else:
        nx.draw_networkx(
            G, pos=pos, arrowsize=20, node_color="#adade0", ax=ax
        )


# Comes from 01c Plotting Utils.ipynb, cell
import torch
import matplotlib.pyplot as plt
from torch_geometric.utils import to_dense_adj


def visualize_heatmap(edge_index, order_ind=None, cmap = "copper", ax=None):
    dense_adj = to_dense_adj(edge_index)[0]
    if order_ind is not None:
        dense_adj = dense_adj[order_ind, :][:, order_ind]
    if ax is not None:
        ax.imshow(dense_adj, cmap=cmap)
    else:
        plt.imshow(dense_adj, cmap=cmap)
        plt.show()


# Comes from 01c Plotting Utils.ipynb, cell
from .datasets import display_galary
import torch
def display_heatmap_galary(dataset, ncol=4):
    vizset = []
    for name, data in dataset:
        order_ind = None if data.y is None else torch.argsort(data.y[:,1])
        vizset.append((name, data, lambda data, ax: visualize_heatmap(data.edge_index, order_ind, ax=ax), False))
    display_galary(vizset, ncol)

# Comes from 01c Plotting Utils.ipynb, cell
def display_graph_galary(dataset, ncol=4):
    vizset = []
    for name, data in dataset:
        vizset.append((name, data, lambda data, ax: visualize_graph(data, ax=ax), False))
    display_galary(vizset, ncol)