# AUTOGENERATED! DO NOT EDIT! File to edit: 03d Grid Sampler.ipynb (unless otherwise specified).

__all__ = ['GaussianVectorField', 'precomputed_distance_loss', 'directed_neighbors', 'smoothness_of_vector_field',
           'kl_divergence_loss', 'compute_grid', 'diffusion_matrix_with_grid_points']

# Cell
import torch.nn as nn
class GaussianVectorField(nn.Module):
  def __init__(self,n_dims, n_gaussians, device, random_initalization = True):
    super(GaussianVectorField, self).__init__()
    self.n_dims = n_dims
    # each gaussian has a mean and a variance, which are initialized randomly, but
    # are afterwards tuned by the network
    self.means = torch.nn.Parameter(torch.rand(n_gaussians,n_dims)*8 - 4).to(device)
    if random_initalization:
      vecs = torch.randn(n_gaussians,n_dims)
    else:
      vecs = torch.ones(n_gaussians,n_dims)
    vecs = vecs / torch.linalg.norm(vecs, dim=1)[:,None]
    self.vectors = torch.nn.Parameter(vecs).to(device)
  def forward(self,points):
    # evaluates the vector field at each point
    # First, take distances between the points and the means
    dist_between_pts_and_means = torch.cdist(points,self.means)
    # print("distances between points and means",dist_between_pts_and_means)
    # apply kernel to this
    # creates n_points x n_means array
    kernel_from_mean = torch.exp(-(dist_between_pts_and_means**2))
    # print("kernalized",kernel_from_mean)
    # multiply kernel value by vectors associated with each Gaussian
    kernel_repeated = kernel_from_mean[:,:,None].repeat(1,1,self.n_dims)
    # print('kernel repeated has shape',kernel_repeated.shape, 'and vecs has shape', self.vectors.shape)
    kernel_times_vectors = kernel_repeated * self.vectors
    # creates tensor of shape
    # n_points x n_means x n_dims
    # collapse along dim 1 to sum vectors along dimension
    vector_field = kernel_times_vectors.sum(dim=1)
    return vector_field

# Cell
import torch
def precomputed_distance_loss(precomputed_distances, embedded_points):
    D_graph = precomputed_distances
    num_nodes = embedded_points.shape[0]
    D_embedding = torch.cdist(embedded_points, embedded_points)
    loss = torch.norm(D_graph - D_embedding)**2 / (num_nodes**2)
    return loss

# Cell
import torch
def directed_neighbors(num_nodes, P_graph, n_neighbors=5):
    # remove self loop
    P_graph = P_graph - torch.eye(num_nodes).to(P_graph.device)
    # return k nearest neighbor indices
    _, neighbors = torch.topk(P_graph, n_neighbors)
    # convert to edge_index format
    row = torch.arange(num_nodes).repeat_interleave(n_neighbors).to(P_graph.device)
    col = neighbors.flatten().to(P_graph.device)
    return torch.stack((row, col))

# Cell
from .data_processing import anisotropic_kernel
def smoothness_of_vector_field(embedded_points, vector_field_function, device, use_grid = True, grid_width = 20):
    if use_grid:
        # find support of points
        minx = (min(embedded_points[:,0])-1).detach()
        maxx = (max(embedded_points[:,0])+1).detach()
        miny = (min(embedded_points[:,1])-1).detach()
        maxy = (max(embedded_points[:,1])+1).detach()
        # form grid around points
        x, y = torch.meshgrid(torch.linspace(minx,maxx,steps=grid_width),torch.linspace(miny,maxy,steps=grid_width))
        xy_t = torch.concat([x[:,:,None],y[:,:,None]],dim=2).float()
        xy_t = xy_t.reshape(grid_width**2,2).to(device)
        points_to_test = xy_t
    else:
        points_to_test = embedded_points
    # Compute distances between points
    # TODO: Can compute A analytically for grid graph, don't need to run kernel
    Dists = torch.cdist(points_to_test,points_to_test)
    A = anisotropic_kernel(Dists)
    # Get degree matrix and build graph laplacian
    D = A.sum(axis=1)
    L = torch.diag(D) - A
    # compute vector field at each grid point
    vecs = vector_field_function(points_to_test)
    x_vecs = vecs[:,0]
    y_vecs = vecs[:,1]
    # compute smoothness of each x and y and add them # TODO: There are other ways this could be done
    x_smoothness = (x_vecs.T @ L @ x_vecs) / torch.max(torch.linalg.norm(x_vecs)**2, torch.tensor(1e-5))
    y_smoothness = (y_vecs.T @ L @ y_vecs) / torch.max(torch.linalg.norm(y_vecs)**2, torch.tensor(1e-5))
    total_smoothness = x_smoothness + y_smoothness
    return total_smoothness

# Cell
def kl_divergence_loss(KLD, P_graph, P_embedding):
    P_embedding = torch.log(P_embedding)
    if KLD.log_target:
        P_graph = torch.log(P_graph)
    if P_embedding.is_sparse:
        P_embedding = P_embedding.to_dense()
        P_graph = P_graph.to_dense()
    loss = KLD(P_embedding, P_graph)
    return loss

# Cell
from .data_processing import (
    affinity_matrix_from_pointset_to_pointset
)
def compute_grid(X, grid_width=20):
    """Returns a grid of points which bounds the points X.
    The grid has 'grid_width' dots in both length and width.
    Accepts X, tensor of shape n x 2
    Returns tensor of shape grid_width^2 x 2"""
    # TODO: This currently only supports
    # find support of points
    minx = float(torch.min(X[:, 0]) - 0.1)  # TODO: use torch.min, try without detach
    maxx = float(torch.max(X[:, 0]) + 0.1)
    miny = float(torch.min(X[:, 1]) - 0.1)
    maxy = float(torch.max(X[:, 1]) + 0.1)
    # form grid around points
    x, y = torch.meshgrid(
        torch.linspace(minx, maxx, steps=grid_width),
        torch.linspace(miny, maxy, steps=grid_width),
        indexing="ij",
    )
    xy_t = torch.concat([x[:, :, None], y[:, :, None]], dim=2).float()
    xy_t = xy_t.reshape(grid_width**2, 2).detach()
    return xy_t

# Cell
import torch
from .data_processing import (
    affinity_matrix_from_pointset_to_pointset,
)
import torch.nn.functional as F

def diffusion_matrix_with_grid_points(X, grid, flow_function, t, sigma, flow_strength):
    n_points = X.shape[0]
    # combine the points and the grid
    points_and_grid = torch.concat([X, grid], dim=0)
    # get flows at each point
    flow_per_point = flow_function(points_and_grid)
    # take a diffusion matrix
    A = affinity_matrix_from_pointset_to_pointset(
        points_and_grid,
        points_and_grid,
        flow=flow_per_point,
        sigma=sigma,
        flow_strength=flow_strength,
    )
    P = F.normalize(A, p=1, dim=-1)
    # TODO: Should we remove self affinities? Probably not, as lazy random walks are advantageous when powering
    # Power the matrix to t steps
    Pt = torch.matrix_power(P, t)
    # Recover the transition probabilities between the points, and renormalize them
    Pt_points = Pt[:n_points, :n_points]
    # Pt_points = torch.diag(1/Pt_points.sum(1)) @ Pt_points
    Pt_points = F.normalize(Pt_points, p=1, dim=1)
    # return diffusion probs between points
    return Pt_points
