# AUTOGENERATED! DO NOT EDIT! File to edit: 05e01 sc inference via integration.ipynb (unless otherwise specified).

__all__ = ['flow_integration_v1', 'flow_integration', 'diffusion_flow_integration', 'plot_flow_line']

# Cell
def flow_integration_v1(X, V, starting_index, step_size, num_steps, min_step_size=0):
    """Returns a list of points along a flow line starting from `starting_index`.

    Parameters
    ----------
    X : torch tensor
        the embedded coordinates
    V : torch tensor
        the velocities associated with each coordinate
    starting_index : int
        the point to start the integration from
    step_size : real
        the maximum size of each step during the integration.
    num_steps : int
        number of steps to take, also number of points to return.
    """
    xi = X[starting_index]
    vi = V[starting_index]
    flow_indices = [starting_index]
    for stepnum in range(num_steps):
        # filter to just points within t*vi meters from xi (not including xi itself)
        distance_to_xi = torch.linalg.norm(X - xi, dim=1)
        # print(f"norm shape {distance_to_xi.shape}")
        filtered = ((distance_to_xi < torch.linalg.norm(vi*step_size)) & (distance_to_xi > torch.linalg.norm(vi*min_step_size)))
        # print(f"filtered shape {filtered.shape}")
        filtered_X  = X[filtered]
        assert len(filtered_X) > 0
        # change of coordinates to have xi at center
        filtered_X = filtered_X - xi
        # Get point which lies closest to the line projected by vi
        normalized_vis = torch.tile(vi/torch.linalg.norm(vi),(len(filtered_X),1))
        print("filtering ",torch.sum(filtered_X*normalized_vis,axis=1))
        filtered_forward = (torch.sum(filtered_X*normalized_vis,axis=1) > 0)
        filtered_X = filtered_X[filtered_forward]
        # print(torch.linalg.norm(filtered_X, axis=1)**2)
        normalized_vis = torch.tile(vi/torch.linalg.norm(vi),(len(filtered_X),1))
        closest_to_line = torch.argmin(torch.linalg.norm(filtered_X, axis=1)**2 - torch.sum(filtered_X*normalized_vis,axis=1))
        # reverse engineer index of closest_to_line in X
        idx = torch.arange(len(X))[filtered][filtered_forward][closest_to_line].detach().item()
        flow_indices.append(idx)
        # set new xi and vi
        xi = X[idx]
        vi = V[idx]
    return flow_indices

# Cell
import numpy as np
import torch
from sklearn.metrics import pairwise_distances
def flow_integration(X, V, starting_index, num_steps, step_size = "automatic", normalize_velocities = True):
    """Returns a list of points along a flow line starting from `starting_index`.

    Parameters
    ----------
    X : torch tensor
        the embedded coordinates
    V : torch tensor
        the velocities associated with each coordinate
    starting_index : int
        the point to start the integration from
    step_size : real
        the maximum size of each step during the integration.
    num_steps : int
        number of steps to take, also number of points to return.
    """
    if step_size == "automatic":
        # use the median distance to 10th nearest neighbor as proxy
        k = 10
        Dists = pairwise_distances(X)
        step_size = np.median(np.partition(Dists,k)[:,k])
        # print("using step size ",step_size)
    xi = X[starting_index]
    vi = V[starting_index]/torch.linalg.norm(V[starting_index])
    idx = starting_index
    flow_indices = [starting_index]
    for stepnum in range(num_steps):
        xj = xi + step_size*vi
        # print(f"step size is {torch.linalg.norm(step_size*vi)}. xj is {xj}. vi is {vi}. xi is {xi}")
        # filter out the base point (from the future: this causes unnecessary oscillation)
        # X_filtered = torch.concat((X[:idx], X[idx+1:]), dim=0)
        # get new index and adjust
        new_idx = torch.argmin(torch.linalg.norm(X - xj, axis=1))
        if new_idx >= idx: new_idx += 1
        idx = new_idx
        # set new xi and vi
        flow_indices.append(idx)
        xi = X[idx]
        vi = V[idx]/torch.linalg.norm(V[idx])
    return flow_indices

# Cell
from .data_processing import flashlight_affinity_matrix
import torch.nn.functional as F
def diffusion_flow_integration(X, V, starting_index, num_steps, sigma="automatic", flow_strength=10):
    # integrate along the manifold, but using the diffusion matrix rather than brute integration along flow arrows
    # preferred to the `flow_integrate` function if doing actual inference from FRED's output, as the diffusion driven integration process better mimics his loss function.
    A = flashlight_affinity_matrix(X,V, sigma=sigma, flow_strength=flow_strength)
    P = F.normalize(A, p=1, dim=1)
    # Starting distribution is the dirac at starting_index
    dist = torch.zeros(len(X))
    dist[starting_index] = 1
    # Diffuse dist by right multiplication by diffusion matrix.
    # On each diffusion, capture the highest probability point
    flow_indices = []
    for t in range(num_steps):
        dist = dist @ P
        flow_indices.append(torch.argmax(dist))
    return flow_indices

# Cell
from .datasets import plot_directed_2d
def plot_flow_line(X,V,flowline):
    Xembedded = X.detach().numpy()
    Vembedded = V.detach().numpy()
    # convert these into labels
    lbs = np.ones(len(Xembedded))
    lbs[flowline] = 0 #+ np.arange(len(flowline))/len(flowline)
    plot_directed_2d(Xembedded,Vembedded,lbs, equal_aspect_ratio=False, cmap="plasma")